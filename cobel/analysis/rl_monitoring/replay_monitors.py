# basic imports
import numpy as np
import pyqtgraph as pg
import PyQt5 as qt


class ReplayMonitor():
    
    def __init__(self, gui_parent: pg.GraphicsWindow, visual_output: bool):
        '''
        Replay monitor. Used for tracking replay sequences at the begin and end of trials.
        
        Parameters
        ----------
        gui_parent :                        The main window for visualization.
        visual_output :                     If true, the learning progress will be plotted.
        
        Returns
        ----------
        None
        '''
        # store the GUI parent
        self.gui_parent = gui_parent
        # shall visual output be provided?
        self.visual_output = visual_output
        # replays are tracked at trial begin (step count == 0) and at trial end.
        self.replay_traces = {'trial_begin': [], 'trial_end': []}
        
        if self.visual_output:
            pass

    def clear_plots(self):
        '''
        This function clears the plots generated by the performance monitor.
        
        Parameters
        ----------
        None
        
        Returns
        ----------
        None
        '''
        if self.visual_output:
            pass
    
    def update(self, logs: dict):
        '''
        This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
        
        Parameters
        ----------
        logs :                              Information from the reinforcement learning subsystem.
        
        Returns
        ----------
        None
        '''
        # determine whether replay occurred at the begin or end of a trial
        replay_time = 'trial_begin' if logs['steps'] == 0 else 'trial_end'
        # store replay trace
        self.replay_traces[replay_time].append(logs['replay'])
        
        if self.visual_output:
            pass
            # update Qt visuals
            if hasattr(qt.QtGui, 'QApplication'):
                qt.QtGui.QApplication.instance().processEvents()
            else:
                qt.QtWidgets.QApplication.instance().processEvents()


class OptimalityMonitor():
    
    def __init__(self, trials: int, gui_parent: pg.GraphicsWindow, visual_output: bool):
        '''
        Optimality monitor. Used for tracking the optimality of replayed experiences.
        
        Parameters
        ----------
        trials :                            Maximum number of trials for which the experiment is run.
        gui_parent :                        The main window for visualization.
        visual_output :                     If true, the learning progress will be plotted.
        
        Returns
        ----------
        None
        '''
        # store the GUI parent
        self.gui_parent = gui_parent
        # shall visual output be provided?
        self.visual_output = visual_output
        # track gain
        self.gain_trace = np.zeros(trials, dtype='float')
        # track optimality
        self.optimality_trace = np.zeros(trials, dtype='float')
        # action selection policy
        self.policy = 'greedy'
        
        if self.visual_output:
            pass

    def clear_plots(self):
        '''
        This function clears the plots generated by the performance monitor.
        
        Parameters
        ----------
        None
        
        Returns
        ----------
        None
        '''
        if self.visual_output:
            pass
        
    def update_local_Q(self, logs: dict):
        '''
        This function is called when replay begins.
        
        Parameters
        ----------
        logs :                              Information from the reinforcement learning subsystem.
        
        Returns
        ----------
        None
        '''
        # store copy 
        self.Q = np.copy(logs['rl_parent'].Q)
    
    def update(self, logs: dict):
        '''
        This function is called when replay ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
        
        Parameters
        ----------
        logs :                              Information from the reinforcement learning subsystem.
        
        Returns
        ----------
        None
        '''
        # determine optimality of replayed experiences
        optimal_replays, gain_cumulative, valid = 0., 0., 0.
        for experience in logs['replay']:
            # determine the utilities of optimal replays
            utility_optimal, ignore = self.compute_optimal_utility(logs)
            # determine the utility of the SFMA replay
            q_old = np.copy(self.Q[experience['state']])
            policy_old = self.compute_action_probs_single(q_old, logs['rl_parent'].beta, logs['rl_parent'].action_mask[experience['state']])
            q_new = np.copy(q_old)
            td = experience['reward'] + logs['rl_parent'].gamma * np.amax(self.Q[experience['next_state']])
            td -= q_new[experience['action']]
            q_new[experience['action']] += logs['rl_parent'].learning_rate * td
            policy_new = self.compute_action_probs_single(q_new, logs['rl_parent'].beta, logs['rl_parent'].action_mask[experience['state']])
            # compute gain of update
            gain = np.sum(policy_new * q_new) - np.sum(policy_old * q_new)
            # update local copy of Q
            self.Q[experience['state'], experience['action']] += logs['rl_parent'].learning_rate * td
            # SFMA replay is optimal if its utility is optimal
            if not ignore:
            #    print(utility_optimal, utility_sfma, np.amax(q_old), np.amax(q_new))
                optimal_replays += float(utility_optimal == gain)
                valid += 1.
            gain_cumulative += gain
        self.optimality_trace[logs['trial']] = optimal_replays#/max(valid, 1)
        self.gain_trace[logs['trial']] = gain_cumulative
        
        if self.visual_output:
            pass
            # update Qt visuals
            if hasattr(qt.QtGui, 'QApplication'):
                qt.QtGui.QApplication.instance().processEvents()
            else:
                qt.QtWidgets.QApplication.instance().processEvents()
    
    def compute_optimal_utility(self, logs: dict) -> (float, bool):
        '''
        This function computes the utility of the optimal experience given the current Q-function.
        
        Parameters
        ----------
        logs :                              Information from the reinforcement learning subsystem.
        
        Returns
        ----------
        gain_optimal :                      The optimal gain.
        invalid :                           Flag indicating whether the gain qualifies as "optimal".
        '''
        updates = np.tile(self.Q, (self.Q.shape[1], 1))
        # bootstrap target values
        next_states = logs['rl_parent'].M.states.flatten(order='F')
        targets = self.Q[next_states]
        targets -= (logs['rl_parent'].action_mask[next_states] == False) * (np.abs(np.amax(targets)) + 1)
        # prepare target mask
        target_mask = np.zeros(updates.shape)
        for action in range(self.Q.shape[1]):
            target_mask[(self.Q.shape[0] * action):(self.Q.shape[0] * (action + 1)), action] = 1.
        # compute updated Q-values
        Q_new = np.copy(updates)
        Q_new += logs['rl_parent'].learning_rate * target_mask * (np.tile(logs['rl_parent'].M.rewards, (self.Q.shape[1], 1)) + logs['rl_parent'].gamma * np.amax(targets, axis=1).reshape(targets.shape[0], 1) - Q_new)
        # compute policies pre and post update
        policy_old = self.compute_action_probs(updates, logs['rl_parent'].beta, np.tile(logs['rl_parent'].action_mask, (self.Q.shape[1], 1)))
        policy_new = self.compute_action_probs(Q_new, logs['rl_parent'].beta, np.tile(logs['rl_parent'].action_mask, (self.Q.shape[1], 1)))
        # comute gain for all updates
        gain = np.sum(policy_new * Q_new, axis=1) - np.sum(policy_old * Q_new, axis=1)
        # remove masked/invalid updates
        gain = gain[logs['rl_parent'].action_mask.flatten()]
        
        return np.amax(gain), int(np.sum(gain == np.amax(gain))) == gain.shape[0]
    
    def compute_action_probs_single(self, q_values: np.ndarray, beta: float = 0, action_mask: np.ndarray = None) -> np.ndarray:
        '''
        This function computes the action selection probabilities for a single set of Q-values.
        
        Parameters
        ----------
        q_values :                          The Q-values.
        beta :                              The inverse temperature parameter used when computing the action selection probabilities under a softmax policy.
        action_mask :                       The action mask (optional).
        
        Returns
        ----------
        probs :                             The action selection probabilities.
        '''
        # assume random policy by default
        actions = np.arange(q_values.shape[0])
        probs = np.ones(q_values.shape)
        # mask actions
        if not action_mask is None:
            probs[action_mask == False] = 0.
            actions = actions[action_mask]
            q_values = q_values[action_mask]
        # compute action selection probabilities
        if self.policy == 'greedy':
            probs[actions] = (q_values == np.amax(q_values))
        elif self.policy == 'softmax':
            q_values -= np.amax(q_values)
            probs[actions] = np.exp(beta * q_values)
        
        return probs/np.sum(probs)
    
    def compute_action_probs(self, Q: np.ndarray, beta: float = 0., action_mask: np.ndarray = None) -> np.ndarray:
        '''
        This function computes the action selection probabilities for all updated Q-values.
        
        Parameters
        ----------
        Q :                                 The Q-values.
        beta :                              The inverse temperature parameter used when computing the action selection probabilities under a softmax policy.
        action_mask :                       The action mask (optional).
        
        Returns
        ----------
        probs :                             The action selection probabilities.
        '''
        # assume random policy by default
        P = np.ones(Q.shape)
        if action_mask is None:
            action_mask = np.ones(P.shape)
        # compute action selection probabilities
        if self.policy == 'greedy':
            Q_masked = Q - (action_mask == False) * (np.abs(np.amax(Q)) + 1)
            P = Q_masked == np.amax(Q_masked, axis=1).reshape(Q.shape[0], 1)
        elif self.policy == 'softmax':
            Q_mask = np.exp(Q * beta)
            P = Q_mask * action_mask
    
        return P/np.sum(P, axis=1).reshape(P.shape[0], 1) 
